# 概念

## 信息的量化度量

:::tip
**比特**： 度量信息量的基本单位。如果一个黑盒子里有A和B两种可能性，它们出现的概率相同，那么要搞清楚到底是A还是B，所需要的信息量就是一比特。
:::

我们把这样充满不确定性的黑盒子就叫做**信息源**，它里面的不确定性叫做**信息熵**，而**信息**就是用来消除这些不确定性的（信息熵），所以搞清楚黑盒子里是怎么一回事，需要的**信息量**就等于黑盒子里的**信息熵**。

**信息熵**公式：

$$H=-p_1\log_2 p_1 - p_2\log_2 p_2 - ... - p_n\log_2 p_n$$

![An image](~@alias/theory/comentropy.png)


## 冗余度

**(信息的编码长度 - 一条信息的信息量)/信息的编码长度**

- 好处：易理解、消除歧义、容错性

- 问题：造成信息存储和传输的浪费、在有噪音的情况下，可能导致混淆


## 等价性和信息增量：信息压缩

1. 对第一位信号完全编码

2. 对第二位即以后的信号，将它和上一个信号相比较，对差异(也被称为增量)进行增量编码



# 香农定理

## 第一定理

**可变长无失真信源编码定理**

给定熵为H(X)的离散无记忆信源及有D个元素的码符号集,总可以找到一种无失真编码方法构成唯一可译码使其平均码长满足:
$$\frac{H(X)}{\log_2 D} \le \bar{n} < 1 + \frac{H(X)}{\log_2 D}$$
(此时为最佳码,$\bar{n} > 1 + \frac{H(X)}{\log_2 D}$时也能找到唯一可译码)，即：

​**编码长度 $\ge$ 信息熵（信息量）/ 每一个码的信息量**




# 哈夫曼编码

**最短编码**

哈夫曼编码从本质上讲，是将最宝贵的资源(最短的编码)给出现概率最大的信息。对于资源如何分配，哈夫曼给出了一个原则，也就是一条信息编码的长度和出现概率的对数成正比。

哈夫曼编码首先会使用字符的频率创建一棵树(最优二叉树)，然后通过这个树的结构为每个字符生成一个特定的编码，出现频率高的字符使用较短的编码，出现频率低的则使用较长的编码，这样就会使编码之后的字符串平均长度降低，从而达到数据无损压缩的目的。



